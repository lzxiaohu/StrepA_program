{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6297657-9bd2-4683-9746-1215e9ad81ca",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "non-default argument follows default argument (functions_list.py, line 2220)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  File \u001b[1;32m/opt/anaconda3/envs/pylfire1_env/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3508\u001b[0m in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001b[0m\n",
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 18\u001b[0;36m\n\u001b[0;31m    import functions_list\u001b[0;36m\n",
      "\u001b[0;36m  File \u001b[0;32m~/Library/CloudStorage/OneDrive-Personal/Research_Knowledge/Projects/StrepA/StrepA_program/code/functions_list.py:2220\u001b[0;36m\u001b[0m\n\u001b[0;31m    def simulator_v5(AgentCharacteristics, ImmuneStatus, params,\u001b[0m\n\u001b[0m                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m non-default argument follows default argument\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Calibrate StrepA ABM with LFIRE using simulation-step indices \n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")  # headless\n",
    "\n",
    "import elfi\n",
    "import pylfire\n",
    "from numpy.random import default_rng, SeedSequence\n",
    "from numpy.random import Generator as _NpGen, RandomState as _RS\n",
    "\n",
    "from pylfire.classifiers.classifier import Classifier\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "import functions_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4130753-46dd-454c-a86a-e42c1230eb24",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 0) Fixed hyperparameters (order MUST match functions_list.parameters())\n",
    "# parameters(): [DurationSimulation, Nstrains, Dimmunity, sigma, omega, x,\n",
    "#                Cperweek, Nagents, alpha, AgeDeath, BasicReproductionNumber]\n",
    "# ----------------------------\n",
    "DurationSimulation = 20.0       # years\n",
    "Nstrains = 42\n",
    "Dimmunity = 0.5 * 52.14         # weeks\n",
    "omega = 0.1\n",
    "x = 10.0\n",
    "Cperweek = 34.53\n",
    "Nagents = 2500\n",
    "alpha = 3.0\n",
    "AgeDeath = 71.0\n",
    "\n",
    "\n",
    "# R0_LOW,  R0_HIGH  = 0.0, 5.0  # reproductionNumber \n",
    "# SIG_LOW, SIG_HIGH = 0.5, 1.0  # sigma "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ce74b2-cf55-4c9d-8df7-6548df061d2b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 1) Observations: treat these as SIMULATION STEP INDICES directly\n",
    "#    (no conversions). Any index outside the simulated range is dropped.\n",
    "# ----------------------------\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"./data/data_real.csv\")\n",
    "df_array = df.astype(float).to_numpy()\n",
    "y_obs_array = df_array.T \n",
    "consultations = np.asarray([27,21,42,51,36,69,\n",
    "                 122,149,172,170,142,147, \n",
    "                 40,193,183,211,190,182, \n",
    "                 199,130,191,188,161]).ravel()\n",
    "# print(y_obs_array)\n",
    "# print(y_obs_array_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2cdb4fb3-5ac1-4ec9-b24b-5289a99991f4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 2) Priors for (BasicReproductionNumber R0, sigma)\n",
    "# ----------------------------\n",
    "R0_LOW,  R0_HIGH  = 0.0, 5.0\n",
    "SIG_LOW, SIG_HIGH = 0.5, 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b54966d4-d3b3-4279-99ae-ec7397f472d0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 3) Build ABM param vector (theta = [R0, sigma])\n",
    "# ----------------------------\n",
    "def build_params(theta2):\n",
    "    th = np.asarray(theta2, float).ravel()\n",
    "    if th.size < 2:\n",
    "        raise ValueError(f\"theta2 must be length-2, got {np.shape(theta2)}\")\n",
    "    R0, sigma = float(th[0]), float(th[1])\n",
    "    return np.array([\n",
    "        DurationSimulation, Nstrains, Dimmunity, sigma, omega, x,\n",
    "        Cperweek, Nagents, alpha, AgeDeath, R0\n",
    "    ], dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a0479c1-4cec-4039-bdca-119eb31ce238",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 4) One ABM run → full prevalence series (length T)\n",
    "# ----------------------------\n",
    "def simulate_prevalence_v5(theta2, seed=123):\n",
    "    # derive a child seed from ELFI's rng\n",
    "    rng = default_rng(seed)\n",
    "    params = build_params(theta2)\n",
    "    AC, IMM, _ = functions_list.initialise_agents_v5(params, rng=rng)\n",
    "\n",
    "    # call the reproducible simulator that uses only this seed\n",
    "    SSPrev_selected, SSPrev, AIBKS = functions_list.simulator_v5(\n",
    "        AC, IMM, params, rng, 0, 1\n",
    "    )\n",
    "\n",
    "    # Option A: return the 42x23 matrix (strain × selected times)\n",
    "    return SSPrev_selected.astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b3743f0-7e01-4e06-ba97-c7043b5e8b41",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T's size 966\n",
      "True True\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# 5) Filter indices to those available from the simulator; align y_obs\n",
    "# ----------------------------\n",
    "# Dry run to learn T\n",
    "\n",
    "_Tdry = simulate_prevalence_v5(np.array([2.0, 1.0], float))\n",
    "T = _Tdry.size \n",
    "print(\"T's size\", T)\n",
    "_Tdry1 = simulate_prevalence_v5(np.array([2.0, 1.0], float))\n",
    "print(np.allclose(_Tdry, _Tdry1), _Tdry.shape == _Tdry1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3a53aa5-dae5-4cf0-aefe-c5bb20dcdc2b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  5.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  1.  0.  0.  1.  0.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  2.  1.  0.  1.  2.  1.  5.  1.  8.  5.  1.  4.  8.  5.  5.  4.\n",
      "   9.  3.  6.  8.  9.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  2.  0.  2.  2.  5.\n",
      "   6. 10. 18.  9. 12.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.]\n",
      " [ 3.  0.  0.  0.  2.  1.  5.  3.  4.  2.  2.  3.  1.  3.  1.  7. 11.  6.\n",
      "   3.  4.  8. 12.  7.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.]\n",
      " [ 1.  3.  0.  2.  2.  1.  5.  2.  9.  7.  7.  6.  0.  8.  6.  4.  4.  3.\n",
      "   4.  3.  3.  0.  0.]\n",
      " [ 1.  1.  0.  0.  0.  3.  4. 10.  2.  6.  2.  2.  2.  1.  5.  9.  9.  4.\n",
      "  10.  8.  4.  1.  4.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  2.  1.  0.  3.  4.  7.  5.  2.  0.  0.  0.  1.  2.  2.  3.\n",
      "   1.  2.  3.  4.  5.]\n",
      " [ 0.  0.  1.  0.  0.  2.  2.  4.  7.  4.  3.  0.  2.  3.  3. 11.  6.  6.\n",
      "  12.  5.  2.  1.  5.]\n",
      " [ 0.  0.  1.  4.  0.  2.  3.  1.  1.  0.  2.  8.  4.  7.  5.  5.  3.  4.\n",
      "   1.  0.  1.  1.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  3.  6.  5.  7.  7.  3.  1.  0.  3.  3.  9.  7.  4.\n",
      "   9.  1.  6.  0.  4.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  2.  1.  0.  2.  3.  3. 12.  9.  3.  6.  4.  4.  8.  4.  5.  1.\n",
      "   4.  2.  3.  6.  4.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  3.  3.  3.  2.  7.  3.  5.  3.  7.  2.  3.  2.  6.  6.  3.  5.\n",
      "  11.  3.  5.  7.  4.]\n",
      " [ 0.  0.  1.  1.  3.  2.  2.  1.  0.  3.  1.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.]\n",
      " [ 2.  0.  0.  4.  1.  1.  3.  1.  6.  1.  8. 10.  0.  9.  4.  9.  2.  5.\n",
      "   7.  4.  2.  1.  5.]\n",
      " [ 0.  0.  1.  1.  0.  1.  3.  3.  4.  3.  3.  5.  2.  6.  9.  1.  2.  1.\n",
      "   4.  3.  1.  3.  8.]\n",
      " [ 0.  0.  0.  2.  1.  4.  3.  8.  5.  2.  2.  1.  0.  1.  3.  7.  3.  1.\n",
      "   1.  3.  2.  2.  3.]\n",
      " [ 2.  0.  3.  2.  4.  2.  2.  1.  1.  4.  0.  1.  1.  0.  0.  2.  1.  0.\n",
      "   0.  0.  2.  5.  2.]\n",
      " [ 0.  1.  0.  2.  2.  1.  2.  3. 10.  3.  3.  2.  2.  4.  2.  2.  2.  2.\n",
      "   4.  1.  1.  2.  1.]\n",
      " [ 0.  0.  1.  0.  1.  1.  1.  1.  6. 13.  4.  5.  2.  5.  4.  3.  3.  6.\n",
      "   6.  5.  4.  5.  3.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  2.  3.  2.  9. 11. 10.  6.  7.\n",
      "  11.  0.  3.  4.  3.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  2.  0.  0.  1.  3.  2.  2.  4.  4.  6.  1.  2.  4.  3.  2.  5.\n",
      "   5.  5.  4.  1.  3.]\n",
      " [ 3.  1.  1.  1.  1.  2.  2.  3.  4. 12.  2.  3.  2.  3.  5.  4.  1.  1.\n",
      "   0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  3.  0.  1.  2.  3.  1.  0.  0.  1.  0.  1.  1.  5.  4.  4.\n",
      "   5.  5.  7.  3.  5.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  3.  0.  0.  3.  5.  1.  4.  2.  0.  2.  0.  1.  6.  6.  6.  4.\n",
      "   4.  5.  4.  3.  1.]]\n"
     ]
    }
   ],
   "source": [
    "print(_Tdry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67e159f-cca8-4cbe-a7aa-d484caed182c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 6) Summary function on the selected values\n",
    "# ----------------------------\n",
    "def my_summary(series_1d):\n",
    "    y = np.asarray(series_1d, float).ravel()\n",
    "    n = y.size\n",
    "    if n == 0:\n",
    "        return np.array([np.nan, np.nan, np.nan, np.nan, 0.0], float)\n",
    "    auc   = float(np.trapz(y))\n",
    "    peak  = float(np.max(y))\n",
    "    tpk   = float(np.argmax(y) / max(n, 1))   # relative peak time\n",
    "    last  = float(y[-1])\n",
    "    rough = float(np.sum(np.diff(y)**2)) if n > 1 else 0.0\n",
    "    return np.array([auc, peak, tpk, last, rough], float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a12c8f25-9fe7-44df-bc58-e5cb95f9598f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# *** 1. AvgTimeObsStr\n",
    "# Take SSPrev_obs>0 (presence/absence). Sum across time for each strain ⇒ “how many time points this strain was seen”. Then average over strains that were ever seen.\n",
    "# → “On average, how long (in time points) an observed strain stayed detectable.”\n",
    "def avg_time_obs_str(SSPrev_obs: np.ndarray, *, nan_if_none: bool = True) -> float:\n",
    "    \"\"\"\n",
    "    AvgTimeObsStr: average # of time points a *seen* strain is present (>0).\n",
    "\n",
    "    SSPrev_obs : (n_strains, T) array of counts per strain per time step.\n",
    "    nan_if_none: if True, return np.nan when no strain is ever seen; else 0.0.\n",
    "    \"\"\"\n",
    "    X = np.asarray(SSPrev_obs, dtype=float)\n",
    "    # presence/absence per strain over time\n",
    "    present = (X > 0).astype(np.int32)        # (S, T)\n",
    "    per_strain_counts = present.sum(axis=1)   # (S,)\n",
    "    seen = per_strain_counts[per_strain_counts > 0]\n",
    "    if seen.size == 0:\n",
    "        return float(\"nan\") if nan_if_none else 0.0\n",
    "    return float(seen.mean())\n",
    "\n",
    "# *** 2.  MaxTimeObsStr\n",
    "# From the same per-strain counts: take the maximum.\n",
    "# → “Longest observed persistence (in time points) among all strains.”\n",
    "\n",
    "def max_time_obs_str(SSPrev_obs: np.ndarray, *, nan_if_none: bool = True) -> float:\n",
    "    \"\"\"\n",
    "    MaxTimeObsStr: longest observed persistence (in time points) among all strains.\n",
    "\n",
    "    SSPrev_obs : (n_strains, T) array of counts per strain per time step.\n",
    "    nan_if_none: if True, return np.nan when no strain is ever seen; else 0.0.\n",
    "    \"\"\"\n",
    "    X = np.asarray(SSPrev_obs, dtype=float)\n",
    "    present = (X > 0).astype(np.int32)        # (S, T) presence/absence\n",
    "    per_strain_counts = present.sum(axis=1)   # time-points seen for each strain\n",
    "    seen = per_strain_counts[per_strain_counts > 0]\n",
    "    if seen.size == 0:\n",
    "        return float(\"nan\") if nan_if_none else 0.0\n",
    "    return float(seen.max())\n",
    "\n",
    "\n",
    "# *** 3. NumStrainsObs\n",
    "# Count how many strains were ever observed (any nonzero across time).\n",
    "# → “Total observed strain richness.”\n",
    "\n",
    "def num_strains_obs_str(SSPrev_obs: np.ndarray) -> int:\n",
    "    \"\"\"\n",
    "    Count how many strains were ever observed (any nonzero across time).\n",
    "\n",
    "    SSPrev_obs : (n_strains, T) array of counts per strain per time step.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        Number of strains with at least one nonzero entry.\n",
    "    \"\"\"\n",
    "    X = np.asarray(SSPrev_obs)\n",
    "    return int(np.any(X > 0, axis=1).sum())\n",
    "\n",
    "# *** 4. AvgTimeRepeatInf\n",
    "# Apply timerepeat(SSPrev_obs): for each strain’s presence/absence row, take indices where present and compute gaps between successive presences; pool all strains’ gaps together and take the mean.\n",
    "# → “Average time between repeat detections (recurrences) across strains.”\n",
    "\n",
    "def avg_time_repeat_inf_numpy(SSPrev_obs: np.ndarray, *, nan_if_empty: bool = True) -> float:\n",
    "    \"\"\"\n",
    "    Average time between repeat detections across strains.\n",
    "    For each strain (row), take presence indices, compute gaps (np.diff),\n",
    "    pool all gaps across strains, then return the mean gap.\n",
    "    \"\"\"\n",
    "    X = (np.asarray(SSPrev_obs, float) > 0)  # (S, T) boolean presence\n",
    "    gaps_all = []\n",
    "    for row in X:\n",
    "        idx = np.flatnonzero(row)           # times where present\n",
    "        if idx.size >= 2:\n",
    "            gaps_all.append(np.diff(idx))   # gaps between presences for this strain\n",
    "    if not gaps_all:\n",
    "        return float(\"nan\") if nan_if_empty else 0.0\n",
    "    gaps = np.concatenate(gaps_all).astype(float)\n",
    "    return float(gaps.mean())\n",
    "\n",
    "# *** 5. VarTimeRepeatInf\n",
    "# Variance of those pooled inter-occurrence gaps.\n",
    "# → “How variable the recurrence intervals are.”\n",
    "\n",
    "def var_time_repeat_inf_numpy(SSPrev_obs, *, nan_if_empty: bool = True) -> float:\n",
    "    \"\"\"\n",
    "    Variance (sample, ddof=1) of pooled gaps between repeat detections across strains.\n",
    "    \"\"\"\n",
    "    X = (np.asarray(SSPrev_obs, float) > 0)  # (S, T) boolean presence\n",
    "    gaps_all = []\n",
    "    for row in X:\n",
    "        idx = np.flatnonzero(row)\n",
    "        if idx.size >= 2:\n",
    "            gaps_all.append(np.diff(idx))\n",
    "    if not gaps_all:\n",
    "        return float(\"nan\") if nan_if_empty else 0.0\n",
    "    gaps = np.concatenate(gaps_all).astype(float)\n",
    "    return float(np.var(gaps, ddof=1)) if gaps.size > 1 else (float(\"nan\") if nan_if_empty else 0.0)\n",
    "\n",
    "\n",
    "# *** 6. AvgPrev\n",
    "# Mean of Prevalence_obs over time.\n",
    "# → “Average prevalence in the observation window.”\n",
    "\n",
    "def avg_prev_numpy(Prevalence_obs) -> float:\n",
    "    \"\"\"\n",
    "    Average prevalence over time.\n",
    "    Accepts array-like (T,) or (T,1). NaNs ignored.\n",
    "    \"\"\"\n",
    "    y = np.asarray(Prevalence_obs, dtype=float).ravel()\n",
    "    return float(np.nanmean(y)) if y.size else float(\"nan\")\n",
    "\n",
    "\n",
    "# *** 7. VarPrev\n",
    "# Variance of Prevalence_obs over time.\n",
    "# → “How much prevalence fluctuates.”\n",
    "\n",
    "def var_prev_numpy(Prevalence_obs, ddof: int = 1) -> float:\n",
    "    \"\"\"\n",
    "    Variance of prevalence over time (ignores NaNs).\n",
    "    Prevalence_obs: array-like (T,) or (T,1)\n",
    "    ddof: 1 for sample variance (MATLAB-like), 0 for population variance.\n",
    "    \"\"\"\n",
    "    y = np.asarray(Prevalence_obs, dtype=float).ravel()\n",
    "    if y.size == 0:\n",
    "        return float(\"nan\")\n",
    "    # need at least 2 valid points for sample variance\n",
    "    if ddof == 1 and np.sum(~np.isnan(y)) < 2:\n",
    "        return float(\"nan\")\n",
    "    return float(np.nanvar(y, ddof=ddof))\n",
    "\n",
    "# *** 8. AvgDiv\n",
    "# Mean of Diversity_obs over time (your diversity metric per time step, e.g., reciprocal Simpson).\n",
    "# → “Average strain diversity over time.”\n",
    "\n",
    "def avg_div_numpy(Diversity_obs) -> float:\n",
    "    \"\"\"\n",
    "    Average strain diversity over time (ignores NaNs).\n",
    "    Diversity_obs: array-like of shape (T,) or (T,1)\n",
    "    \"\"\"\n",
    "    y = np.asarray(Diversity_obs, dtype=float).ravel()\n",
    "    return float(np.nanmean(y)) if y.size else float(\"nan\")\n",
    "    \n",
    "\n",
    "# *** 9. VarDiv\n",
    "# Variance of Diversity_obs.\n",
    "# → “How much strain diversity fluctuates.”\n",
    "\n",
    "def var_div_numpy(Diversity_obs, ddof: int = 1) -> float:\n",
    "    \"\"\"\n",
    "    Variance of diversity over time (ignores NaNs).\n",
    "    Diversity_obs: array-like (T,) or (T,1)\n",
    "    ddof: 1 for sample variance (MATLAB-like), 0 for population variance.\n",
    "    \"\"\"\n",
    "    y = np.asarray(Diversity_obs, dtype=float).ravel()\n",
    "    if y.size == 0:\n",
    "        return float(\"nan\")\n",
    "    if ddof == 1 and np.sum(~np.isnan(y)) < 2:\n",
    "        return float(\"nan\")\n",
    "    return float(np.nanvar(y, ddof=ddof))\n",
    "\n",
    "# *** 10. MaxAbundance\n",
    "# max(max(SSPrev_obs)): the single highest count seen for any strain at any time.\n",
    "# → “Peak abundance of a strain at a time point.”\n",
    "\n",
    "def max_abundance_numpy(SSPrev_obs) -> float:\n",
    "    \"\"\"\n",
    "    Peak abundance of a strain at any time.\n",
    "    SSPrev_obs: array-like (n_strains, T). NaNs ignored.\n",
    "    \"\"\"\n",
    "    X = np.asarray(SSPrev_obs, dtype=float)\n",
    "    if X.size == 0 or np.isnan(X).all():\n",
    "        return float(\"nan\")\n",
    "    return float(np.nanmax(X))\n",
    "\n",
    "# *** 11. AvgNPMI (MI(SSPrev_obs))\n",
    "# Average normalized pointwise mutual information across strain pairs using presence/absence over time. Positive ⇒ co-occur more than chance; ~0 ⇒ independent; negative ⇒ avoid each other.\n",
    "# → “Average pairwise co-occurrence beyond chance.”\n",
    "\n",
    "def avg_npmi_numpy(SSPrev_obs) -> float:\n",
    "    \"\"\"\n",
    "    Average normalized PMI across strain pairs using presence/absence over time.\n",
    "    Skips pairs with p_ij == 0. Returns NaN if no valid pairs.\n",
    "    \"\"\"\n",
    "    X = (np.asarray(SSPrev_obs, dtype=float) > 0).astype(np.int32)  # (S, T)\n",
    "    S, T = X.shape\n",
    "    if S < 2 or T == 0:\n",
    "        return float(\"nan\")\n",
    "\n",
    "    # per-strain presence probability\n",
    "    c_i = X.sum(axis=1).astype(float)          # (S,)\n",
    "    p_i = c_i / T\n",
    "\n",
    "    # pairwise co-occurrence counts & probabilities\n",
    "    C = X @ X.T                                # (S, S) co-occurrence counts\n",
    "    iu = np.triu_indices(S, k=1)\n",
    "    c_ij = C[iu].astype(float)\n",
    "    p_ij = c_ij / T\n",
    "\n",
    "    # valid pairs: p_ij>0 and p_i,p_j>0\n",
    "    pi_i = p_i[iu[0]]\n",
    "    pi_j = p_i[iu[1]]\n",
    "    valid = (p_ij > 0) & (pi_i > 0) & (pi_j > 0)\n",
    "    if not np.any(valid):\n",
    "        return float(\"nan\")\n",
    "\n",
    "    pij = p_ij[valid]\n",
    "    pmi = np.log(pij / (pi_i[valid] * pi_j[valid]))\n",
    "    npmi = pmi / (-np.log(pij))\n",
    "    return float(np.mean(npmi))\n",
    "\n",
    "\n",
    "# *** 12. DivAllIsolates\n",
    "# First collapse time by summing counts per strain: sum(SSPrev_obs, 2) (i.e., total isolates per strain across all time). Then compute your diversity function div(...) on that vector.\n",
    "# → “Overall strain diversity across the entire study period (richness + evenness), time-collapsed.”\n",
    "\n",
    "def div_all_isolates_numpy(SSPrev_obs) -> float:\n",
    "    \"\"\"\n",
    "    Overall strain diversity across the entire study period.\n",
    "    Steps:\n",
    "      1) Collapse time: totals per strain = sum over columns (time).\n",
    "      2) Reciprocal Simpson: D = N*(N-1) / sum_i n_i*(n_i-1)\n",
    "         (returns 0 if no isolates; equals richness when all n_i ∈ {0,1})\n",
    "    \"\"\"\n",
    "    X = np.asarray(SSPrev_obs, dtype=float)\n",
    "    if X.size == 0:\n",
    "        return 0.0\n",
    "    totals = X.sum(axis=1)              # per-strain totals across time\n",
    "    N = totals.sum()\n",
    "    if N <= 1:\n",
    "        return 0.0\n",
    "    denom = np.sum(totals * (totals - 1.0))\n",
    "    if denom <= 0:\n",
    "        # all totals are 0 or 1 → diversity equals number of nonzero strains\n",
    "        return float((totals > 0).sum())\n",
    "    return float(N * (N - 1.0) / denom)\n",
    "\n",
    "\n",
    "def my_summary_v4(series_2d):\n",
    "    y = np.asarray(series_2d, float).ravel()\n",
    "    avg_time_obs = avg_time_obs_str(series_2d)\n",
    "    max_time_obs = max_time_obs_str(series_2d)\n",
    "    num_strains_obs = num_strains_obs_str(series_2d)\n",
    "    avg_time_repeat_obs = avg_time_repeat_inf_numpy(series_2d)\n",
    "    var_time_repeat_obs = var_time_repeat_inf_numpy(series_2d)\n",
    "    \n",
    "    return np.array([avg_time_obs, max_time_obs, num_strains_obs, avg_time_repeat_obs, var_time_repeat_obs], float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0e02928-a39e-4478-91b9-38c1bd7fc065",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s_obs [ 3.96551724  8.         29.          2.19767442  5.71340629]\n"
     ]
    }
   ],
   "source": [
    "s_obs_v4 = my_summary_v4(y_obs_array)\n",
    "print(\"s_obs\", s_obs_v4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d387982-8b0e-4678-bb7d-7730ef8a3001",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 7) Robust ELFI random_state → int seed\n",
    "# ----------------------------\n",
    "def _seed_from_random_state(random_state):\n",
    "    if isinstance(random_state, _NpGen):\n",
    "        return int(random_state.integers(0, 2**32 - 1))\n",
    "    if isinstance(random_state, _RS):\n",
    "        return int(random_state.randint(0, 2**32 - 1))\n",
    "    if isinstance(random_state, (int, np.integer)):\n",
    "        return int(random_state)\n",
    "    return int(np.random.randint(0, 2**32 - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7cfceaf-7f76-46ec-be62-660a2dfda213",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 8) ELFI simulator: return simulated values at YOUR indices (idx_valid)\n",
    "# ----------------------------\n",
    "\n",
    "def elfi_simulator_v4(R0, sigma, batch_size=1, random_state=None):\n",
    "    R0    = np.atleast_1d(np.asarray(R0, float))\n",
    "    sigma = np.atleast_1d(np.asarray(sigma, float))\n",
    "    bs = int(max(R0.size, sigma.size, int(batch_size)))\n",
    "    if R0.size == 1:    R0    = np.repeat(R0, bs)\n",
    "    if sigma.size == 1: sigma = np.repeat(sigma, bs)\n",
    "\n",
    "    seed = _seed_from_random_state(random_state)\n",
    "    ss_parent = SeedSequence(seed)\n",
    "    gens = [default_rng(s) for s in ss_parent.spawn(bs)]\n",
    "\n",
    "    outs = []\n",
    "    for i in range(bs):\n",
    "        theta_i = np.array([R0[i], sigma[i]], float)\n",
    "        prev_i = simulate_prevalence_v4(theta_i, gens[i])  # full series (42*23)\n",
    "        outs.append(prev_i)                  # pick your indices only\n",
    "    return np.stack(outs, axis=0)                       # (bs, len(idx_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b6c9258f-f2d7-4d59-811e-a3a9f788c6a5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 9) Build ELFI graph (no context manager)\n",
    "# ----------------------------\n",
    "m = elfi.new_model()\n",
    "R0_node  = elfi.Prior('uniform', R0_LOW,  R0_HIGH  - R0_LOW,  model=m, name='R0')\n",
    "SIG_node = elfi.Prior('uniform', SIG_LOW, SIG_HIGH - SIG_LOW, model=m, name='sigma')\n",
    "sim_node = elfi.Simulator(elfi_simulator_v4, R0_node, SIG_node, model=m, name='sim')\n",
    "summary_node = elfi.Summary(my_summary_v4, sim_node, observed=s_obs_v4, model=m, name='summary')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cb551f17-5e91-4128-8d3e-7b5657b5e87a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 10) Build LFIRE classifier\n",
    "# ----------------------------\n",
    "class SKLogitClassifierAdapter(Classifier):\n",
    "    parallel_cv = False  # tell PYLFIRE not to parallelize via ELFI client\n",
    "\n",
    "    def __init__(self, cv=5, n_jobs=1, max_iter=2000):\n",
    "        # do NOT call super().__init__()\n",
    "        self.cv = int(cv)\n",
    "        self.n_jobs = None if int(n_jobs) == 1 else int(n_jobs)\n",
    "        self.max_iter = int(max_iter)\n",
    "        self.models_ = {}\n",
    "\n",
    "    def fit(self, X, y, index=0):\n",
    "        X = np.asarray(X, float)\n",
    "        y = np.asarray(y)\n",
    "\n",
    "        # Ensure 2D features\n",
    "        if X.ndim == 1:\n",
    "            X = X.reshape(-1, 1)\n",
    "\n",
    "        # Map {-1,+1} -> {0,1} if needed\n",
    "        uniq = np.unique(y)\n",
    "        if set(uniq.tolist()) == {-1, 1}:\n",
    "            y = ((y + 1) // 2).astype(int)\n",
    "\n",
    "        # Effective CV ≤ samples per class (min 2)\n",
    "        n_pos = int((y == 1).sum())\n",
    "        n_neg = int((y == 0).sum())\n",
    "        cv_eff = max(2, min(self.cv, n_pos, n_neg))\n",
    "\n",
    "        lr = LogisticRegressionCV(\n",
    "            cv=cv_eff, max_iter=self.max_iter, solver=\"lbfgs\",\n",
    "            scoring=\"neg_log_loss\", n_jobs=self.n_jobs\n",
    "        )\n",
    "        lr.fit(X, y)\n",
    "        self.models_[index] = lr\n",
    "        return self\n",
    "\n",
    "    def predict_likelihood_ratio(self, obs, index=0):\n",
    "        lr = self.models_[index]\n",
    "        obs = np.asarray(obs, float).ravel()\n",
    "\n",
    "        # Match feature count used in fit\n",
    "        n_feat = int(getattr(lr, \"n_features_in_\", obs.size))\n",
    "        if obs.size != n_feat:\n",
    "            if obs.size > n_feat:\n",
    "                obs = obs[:n_feat]\n",
    "            else:\n",
    "                obs = np.pad(obs, (0, n_feat - obs.size), mode=\"constant\", constant_values=0.0)\n",
    "\n",
    "        obs = obs.reshape(1, -1)\n",
    "        proba = lr.predict_proba(obs)[0]\n",
    "        classes = lr.classes_\n",
    "        pos_idx = int(np.where(classes == 1)[0][0]) if 1 in classes else 1\n",
    "        p1 = float(proba[pos_idx])\n",
    "        p0 = max(1e-12, 1.0 - p1)\n",
    "        return p1 / p0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e86267f2-a593-40d5-85e6-57f9da4de085",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 11) Parameter grid for LFIRE\n",
    "# ----------------------------\n",
    "FAST = 1\n",
    "# ---- fast settings ----\n",
    "if FAST:\n",
    "    Nstrains = 42                # (was 42)\n",
    "    Nagents = 2500               # (was 2500)\n",
    "    # tiny LFIRE grid\n",
    "    R0_grid  = np.linspace(R0_LOW,  R0_HIGH,  5)\n",
    "    SIG_grid = np.linspace(SIG_LOW, SIG_HIGH, 4)\n",
    "    batch_size_lfire = 32       # (was 512)\n",
    "else:\n",
    "    # original grid / batch size\n",
    "    R0_grid  = np.linspace(R0_LOW,  R0_HIGH,  40)\n",
    "    SIG_grid = np.linspace(SIG_LOW, SIG_HIGH, 20)\n",
    "    batch_size_lfire = 64\n",
    "\n",
    "TT1, TT2 = np.meshgrid(R0_grid, SIG_grid, indexing='ij')\n",
    "params_grid = np.c_[TT1.ravel(), TT2.ravel()]  # (40*20, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fd507e01-5a4f-4623-946b-f222903d5f8c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xxx\n",
      "Progress: |██████████████████████████████████████████--------| 85.0% Complete"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 22\u001b[0m\n\u001b[1;32m     14\u001b[0m lfire \u001b[38;5;241m=\u001b[39m pylfire\u001b[38;5;241m.\u001b[39mLFIRE(\n\u001b[1;32m     15\u001b[0m     model\u001b[38;5;241m=\u001b[39mm,\n\u001b[1;32m     16\u001b[0m     params_grid\u001b[38;5;241m=\u001b[39mparams_grid,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     19\u001b[0m     output_names\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msummary\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mR0\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigma\u001b[39m\u001b[38;5;124m'\u001b[39m]  \u001b[38;5;66;03m# <-- add params here\u001b[39;00m\n\u001b[1;32m     20\u001b[0m )\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxxx\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 22\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mlfire\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Downloads/zoo-master/pylfire/pylfire/methods/lfire.py:196\u001b[0m, in \u001b[0;36mLFIRE.infer\u001b[0;34m(self, observed, *args, **kwargs)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# 2. evaluate posterior\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_batches\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams_grid\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n\u001b[0;32m--> 196\u001b[0m     post\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mLFIRE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     post\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_evaluate_posterior()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pylfire1_env/lib/python3.8/site-packages/elfi/methods/parameter_inference.py:264\u001b[0m, in \u001b[0;36mParameterInference.infer\u001b[0;34m(self, vis, bar, *args, **kwargs)\u001b[0m\n\u001b[1;32m    260\u001b[0m     progress_bar(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_objective_n_batches, prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProgress:\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    261\u001b[0m                  suffix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mComplete\u001b[39m\u001b[38;5;124m'\u001b[39m, length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m)\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinished:\n\u001b[0;32m--> 264\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    265\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m vis:\n\u001b[1;32m    266\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mplot_state(interactive\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mvis_opt)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pylfire1_env/lib/python3.8/site-packages/elfi/methods/parameter_inference.py:306\u001b[0m, in \u001b[0;36mParameterInference.iterate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatches\u001b[38;5;241m.\u001b[39msubmit(next_batch)\n\u001b[1;32m    305\u001b[0m \u001b[38;5;66;03m# Handle the next ready batch in succession\u001b[39;00m\n\u001b[0;32m--> 306\u001b[0m batch, batch_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatches\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait_next\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    307\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReceived batch \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m batch_index)\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate(batch, batch_index)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pylfire1_env/lib/python3.8/site-packages/elfi/client.py:177\u001b[0m, in \u001b[0;36mBatchHandler.wait_next\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCannot wait for a batch, no batches currently submitted\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    176\u001b[0m batch_index, task_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pending_batches\u001b[38;5;241m.\u001b[39mpopitem(last\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 177\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReceived batch \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(batch_index))\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext\u001b[38;5;241m.\u001b[39mcallback(batch, batch_index)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pylfire1_env/lib/python3.8/site-packages/elfi/clients/native.py:65\u001b[0m, in \u001b[0;36mClient.get_result\u001b[0;34m(self, task_id)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return the result from task identified by `task_id` when it arrives.\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \n\u001b[1;32m     58\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     62\u001b[0m \n\u001b[1;32m     63\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     64\u001b[0m kallable, args, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtasks\u001b[38;5;241m.\u001b[39mpop(task_id)\n\u001b[0;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mkallable\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pylfire1_env/lib/python3.8/site-packages/elfi/executor.py:70\u001b[0m, in \u001b[0;36mExecutor.execute\u001b[0;34m(cls, G)\u001b[0m\n\u001b[1;32m     68\u001b[0m op \u001b[38;5;241m=\u001b[39m attr[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moperation\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 70\u001b[0m     G\u001b[38;5;241m.\u001b[39mnode[node] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mG\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn executing node \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     73\u001b[0m                         \u001b[38;5;241m.\u001b[39mformat(node, exc))\u001b[38;5;241m.\u001b[39mwith_traceback(exc\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pylfire1_env/lib/python3.8/site-packages/elfi/executor.py:154\u001b[0m, in \u001b[0;36mExecutor._run\u001b[0;34m(fn, node, G)\u001b[0m\n\u001b[1;32m    150\u001b[0m         kwargs[param] \u001b[38;5;241m=\u001b[39m output\n\u001b[1;32m    152\u001b[0m args \u001b[38;5;241m=\u001b[39m [a[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(args, key\u001b[38;5;241m=\u001b[39mitemgetter(\u001b[38;5;241m0\u001b[39m))]\n\u001b[0;32m--> 154\u001b[0m output_dict \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m}\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output_dict\n",
      "Cell \u001b[0;32mIn[14], line 19\u001b[0m, in \u001b[0;36melfi_simulator_v4\u001b[0;34m(R0, sigma, batch_size, random_state)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(bs):\n\u001b[1;32m     18\u001b[0m     theta_i \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([R0[i], sigma[i]], \u001b[38;5;28mfloat\u001b[39m)\n\u001b[0;32m---> 19\u001b[0m     prev_i \u001b[38;5;241m=\u001b[39m \u001b[43msimulate_prevalence_v4\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtheta_i\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgens\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# full series (42*23)\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     outs\u001b[38;5;241m.\u001b[39mappend(prev_i)                  \u001b[38;5;66;03m# pick your indices only\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mstack(outs, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "Cell \u001b[0;32mIn[6], line 9\u001b[0m, in \u001b[0;36msimulate_prevalence_v4\u001b[0;34m(theta2, rng)\u001b[0m\n\u001b[1;32m      7\u001b[0m params \u001b[38;5;241m=\u001b[39m build_params(theta2)\n\u001b[1;32m      8\u001b[0m AC, IMM, _ \u001b[38;5;241m=\u001b[39m functions_list\u001b[38;5;241m.\u001b[39minitialise_agents(params)\n\u001b[0;32m----> 9\u001b[0m SSPrev_selected, SSPrev, AIBKS \u001b[38;5;241m=\u001b[39m \u001b[43mfunctions_list\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimulator_v4\u001b[49m\u001b[43m(\u001b[49m\u001b[43mAC\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIMM\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m prevalence \u001b[38;5;241m=\u001b[39m SSPrev_selected   \u001b[38;5;66;03m# shape (42, 23)\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39masarray(prevalence, \u001b[38;5;28mfloat\u001b[39m)\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-Personal/Research_Knowledge/Projects/StrepA/StrepA_program/code/functions_list.py:1581\u001b[0m, in \u001b[0;36msimulator_v4\u001b[0;34m(AgentCharacteristics, ImmuneStatus, params, specifyPtransmission, cross_immunity_effect_on_coinfections)\u001b[0m\n\u001b[1;32m   1578\u001b[0m     chosen \u001b[38;5;241m=\u001b[39m infecting_strains[idx]\n\u001b[1;32m   1580\u001b[0m \u001b[38;5;66;03m# success Bernoulli\u001b[39;00m\n\u001b[0;32m-> 1581\u001b[0m susc \u001b[38;5;241m=\u001b[39m \u001b[43mInfectionProb\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcontacts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchosen\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m   1582\u001b[0m U3 \u001b[38;5;241m=\u001b[39m _takeU(X)\n\u001b[1;32m   1584\u001b[0m \u001b[38;5;66;03m# Instead of success mask + np.any(success), get indices directly\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# 12) Run LFIRE\n",
    "# ----------------------------\n",
    "clf = SKLogitClassifierAdapter(cv=5, max_iter=2000)\n",
    "\n",
    "# lfire = pylfire.LFIRE(\n",
    "#     model=m,\n",
    "#     params_grid=params_grid,\n",
    "#     batch_size=batch_size_lfire,\n",
    "#     classifier=clf,\n",
    "#     output_names=['summary', 'R0', 'sigma']  # <-- add params here\n",
    "# )\n",
    "\n",
    "lfire = pylfire.LFIRE(\n",
    "    model=m,\n",
    "    params_grid=params_grid,\n",
    "    batch_size=batch_size_lfire,\n",
    "    classifier=clf,\n",
    "    output_names=['summary', 'R0', 'sigma']  # <-- add params here\n",
    ")\n",
    "print(\"xxx\")\n",
    "res = lfire.infer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e60c8c-b8bd-4239-9d11-32ad139cc320",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 13) Save posterior results \n",
    "# ----------------------------\n",
    "def extract_posterior_any(res, lfire_obj, params_grid):\n",
    "    \"\"\"Robustly pull (theta_grid, weights) from PYLFIRE/ELFI results.\"\"\"\n",
    "    theta, w = None, None\n",
    "\n",
    "    # 1) Try dict-like/attr-like result first\n",
    "    if isinstance(res, dict):\n",
    "        theta = res.get(\"theta\") or res.get(\"thetas\") or res.get(\"grid\")\n",
    "        w = res.get(\"posterior\") or res.get(\"weights\") or res.get(\"post\")\n",
    "    else:\n",
    "        theta = getattr(res, \"theta\", None)\n",
    "        w = (getattr(res, \"posterior\", None)\n",
    "             or getattr(res, \"weights\", None)\n",
    "             or getattr(res, \"post\", None)\n",
    "             or (getattr(res, \"results\", {}).get(\"weights\") if hasattr(res, \"results\") else None))\n",
    "\n",
    "    # 2) Pull from lfire.state (most reliable path)\n",
    "    if (theta is None or w is None) and hasattr(lfire_obj, \"state\"):\n",
    "        st = lfire_obj.state\n",
    "        # get parameter names and stack their tracked values into a grid\n",
    "        names = getattr(lfire_obj, \"parameter_names\", None)\n",
    "        if not names:\n",
    "            names = [k for k in (\"R0\", \"sigma\") if k in st]  # sensible default\n",
    "        cols = []\n",
    "        for nm in names:\n",
    "            if nm in st:\n",
    "                cols.append(np.asarray(st[nm]).ravel())\n",
    "        if cols and all(c.size == cols[0].size for c in cols):\n",
    "            theta = np.column_stack(cols)\n",
    "        # posterior array is usually here\n",
    "        w = st.get(\"posterior\", w)\n",
    "\n",
    "    # 3) As a last resort, evaluate posterior now (may be unnecessary)\n",
    "    if (w is None) and hasattr(lfire_obj, \"_evaluate_posterior\"):\n",
    "        try:\n",
    "            ev = lfire_obj._evaluate_posterior()\n",
    "            if isinstance(ev, dict):\n",
    "                w = ev.get(\"posterior\") or ev.get(\"weights\") or ev.get(\"post\")\n",
    "            if theta is None:\n",
    "                theta = params_grid\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # 4) Final fallbacks\n",
    "    if theta is None:\n",
    "        theta = params_grid\n",
    "    theta = np.asarray(theta, float)\n",
    "\n",
    "    if w is None:\n",
    "        # uniform fallback if classifier produced degenerate/hidden weights\n",
    "        w = np.ones(theta.shape[0], float)\n",
    "    else:\n",
    "        w = np.asarray(w, float).ravel()\n",
    "\n",
    "    # clean + normalise\n",
    "    w = np.clip(w, 0, None)\n",
    "    s = np.nansum(w)\n",
    "    w = (np.ones_like(w) / max(w.size, 1)) if (not np.isfinite(s) or s <= 0) else (w / s)\n",
    "    return theta, w\n",
    "\n",
    "# use it\n",
    "theta_grid, weights = extract_posterior_any(res, lfire_obj=lfire, params_grid=params_grid)\n",
    "\n",
    "map_idx    = int(np.argmax(weights))\n",
    "theta_map  = theta_grid[map_idx]\n",
    "theta_mean = (weights[:, None] * theta_grid).sum(axis=0)\n",
    "\n",
    "print(f\"[RESULT] MAP: {theta_map} | Posterior mean: {theta_mean}\")\n",
    "\n",
    "import os, pandas as pd\n",
    "os.makedirs(\"experimental_data\", exist_ok=True)\n",
    "pd.DataFrame(theta_grid, columns=[\"R0\", \"sigma\"]).assign(weight=weights)\\\n",
    "  .to_csv(\"experimental_data/posterior_grid_v4.csv\", index=False)\n",
    "print(\"Saved → experimental_data/posterior_grid_v4.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fdee53-4fd9-4873-a751-e69148006b72",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "#  Heatmap \n",
    "# ----------------------------\n",
    "\n",
    "# Get θ-grid and normalized weights\n",
    "theta_grid = np.column_stack([np.asarray(lfire.state['R0']).ravel(),\n",
    "                              np.asarray(lfire.state['sigma']).ravel()])\n",
    "w = np.asarray(lfire.state.get('posterior'), float).ravel()\n",
    "w = np.clip(w, 0, None); w = w / (w.sum() if w.sum() > 0 else 1.0)\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# or, in pure Python:\n",
    "import matplotlib\n",
    "matplotlib.use('module://matplotlib_inline.backend_inline')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "R0_vals  = np.unique(theta_grid[:,0])\n",
    "SIG_vals = np.unique(theta_grid[:,1])\n",
    "\n",
    "# reshape to 2D if grid is rectangular\n",
    "if R0_vals.size * SIG_vals.size == theta_grid.shape[0]:\n",
    "    post2d = w.reshape(R0_vals.size, SIG_vals.size, order='C')  # 'ij' meshgrid\n",
    "    plt.figure(figsize=(6,4.5))\n",
    "    plt.pcolormesh(SIG_vals, R0_vals, post2d, shading='auto')\n",
    "    plt.xlabel(r'$\\sigma$ (ss-immunity strength)')\n",
    "    plt.ylabel(r'$R_0$')\n",
    "    plt.title('Posterior over $(R_0, \\sigma)$')\n",
    "    plt.colorbar(label='Probability')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('experimental_data/posterior_heatmap_v4.png', dpi=160)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    print(\"Saved → experimental_data/posterior_heatmap_v4.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9faf90a-382d-4745-9567-688aaaee8279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "#  draw samples from the discrete posterior\n",
    "# ----------------------------\n",
    "# \n",
    "rng = np.random.default_rng(0)\n",
    "idx = rng.choice(theta_grid.shape[0], size=5000, replace=True, p=w)\n",
    "samps = theta_grid[idx]\n",
    "R0_s, SIG_s = samps[:,0], samps[:,1]\n",
    "\n",
    "# MAP point\n",
    "map_idx = int(np.argmax(w))\n",
    "theta_map = theta_grid[map_idx]\n",
    "\n",
    "plt.figure(figsize=(6,4.5))\n",
    "plt.scatter(SIG_s, R0_s, s=6, alpha=0.25, edgecolors='none')\n",
    "plt.scatter([theta_map[1]],[theta_map[0]], s=60, marker='x')\n",
    "plt.xlabel(r'$\\sigma$'); plt.ylabel(r'$R_0$'); plt.title('Posterior samples (scatter) with MAP (×)')\n",
    "plt.tight_layout()\n",
    "plt.savefig('experimental_data/posterior_scatter_v4.png', dpi=160)\n",
    "plt.show()\n",
    "plt.close()\n",
    "print(\"Saved → experimental_data/posterior_scatter_v4.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32dd0d22-d6d2-458c-8e57-d7b17fe6e114",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "#  Marginal densities + 95% CIs\n",
    "# ----------------------------\n",
    "# \n",
    "def ci95(a): \n",
    "    lo, hi = np.quantile(a, [0.025, 0.975]); return float(lo), float(hi)\n",
    "\n",
    "lo_R0, hi_R0   = ci95(R0_s)\n",
    "lo_SIG, hi_SIG = ci95(SIG_s)\n",
    "mean_R0  = float(R0_s.mean())\n",
    "mean_SIG = float(SIG_s.mean())\n",
    "\n",
    "# R0\n",
    "plt.figure(figsize=(6,3))\n",
    "plt.hist(R0_s, bins=30, density=True)\n",
    "plt.axvline(mean_R0, color='k', linestyle='--', linewidth=1)\n",
    "plt.axvline(lo_R0,  color='k', linestyle=':',  linewidth=1)\n",
    "plt.axvline(hi_R0,  color='k', linestyle=':',  linewidth=1)\n",
    "plt.title(rf'$R_0$ marginal (mean={mean_R0:.3g}, 95% CI [{lo_R0:.3g}, {hi_R0:.3g}])')\n",
    "plt.xlabel(r'$R_0$'); plt.ylabel('Density'); plt.tight_layout()\n",
    "plt.savefig('experimental_data/posterior_marginal_R0_v4.png', dpi=160)\n",
    "plt.show()\n",
    "plt.close()\n",
    "print(\"Saved → experimental_data/posterior_marginal_R0_v4.png\")\n",
    "\n",
    "# sigma\n",
    "plt.figure(figsize=(6,3))\n",
    "plt.hist(SIG_s, bins=30, density=True)\n",
    "plt.axvline(mean_SIG, color='k', linestyle='--', linewidth=1)\n",
    "plt.axvline(lo_SIG,  color='k', linestyle=':',  linewidth=1)\n",
    "plt.axvline(hi_SIG,  color='k', linestyle=':',  linewidth=1)\n",
    "plt.title(rf'$\\sigma$ marginal (mean={mean_SIG:.3g}, 95% CI [{lo_SIG:.3g}, {hi_SIG:.3g}])')\n",
    "plt.xlabel(r'$\\sigma$'); plt.ylabel('Density'); plt.tight_layout()\n",
    "plt.savefig('experimental_data/posterior_marginal_sigma_v4.png', dpi=160)\n",
    "plt.show()\n",
    "plt.close()\n",
    "print(\"Saved → experimental_data/posterior_marginal_sigma_v4.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6b0a23-1ed1-4183-b736-38d07c168538",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "#  Get posterior over parameters (θ = [R0, σ])\n",
    "# ----------------------------\n",
    "# \n",
    "import numpy as np\n",
    "\n",
    "# θ-grid from LFIRE’s state\n",
    "theta_grid = np.column_stack([\n",
    "    np.asarray(lfire.state['R0']).ravel(),\n",
    "    np.asarray(lfire.state['sigma']).ravel()\n",
    "])\n",
    "\n",
    "# posterior weights (normalize)\n",
    "w = np.asarray(lfire.state.get('posterior'), float).ravel()\n",
    "w = np.clip(w, 0, None)\n",
    "w = w / (w.sum() if w.sum() > 0 else 1.0)\n",
    "\n",
    "# point estimates\n",
    "i_map     = int(np.argmax(w))\n",
    "theta_map = theta_grid[i_map]\n",
    "theta_mean= (w[:, None] * theta_grid).sum(axis=0)\n",
    "\n",
    "print(f\"MAP:   R0={theta_map[0]:.3f}, sigma={theta_map[1]:.3f}\")\n",
    "print(f\"Mean:  R0={theta_mean[0]:.3f}, sigma={theta_mean[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a78a3ad-94b2-4ba7-8e2d-3726ee29ec66",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate fitted curves (MAP & posterior predictive) \n",
    "from numpy.random import default_rng\n",
    "\n",
    "rng = default_rng(0)\n",
    "\n",
    "# helper: simulate & select your time indices\n",
    "def sim_at(theta_2, seed=None):\n",
    "    g = default_rng(seed) if seed is not None else rng\n",
    "    full = simulate_prevalence_v4(np.asarray(theta_2, float), g)\n",
    "    return full   # <- your selected indices only\n",
    "\n",
    "# MAP fit\n",
    "y_hat_map = sim_at(theta_map, seed=123)\n",
    "\n",
    "# Posterior predictive: sample θ from grid according to w, simulate many times\n",
    "n_ppc = 500   # increase later for smoother bands\n",
    "idxs = rng.choice(theta_grid.shape[0], size=n_ppc, replace=True, p=w)\n",
    "ppc  = np.stack([sim_at(theta_grid[i], seed=int(rng.integers(0, 2**32-1)))\n",
    "                 for i in idxs], axis=0)    # shape (n_ppc, L)\n",
    "y_hat_mean = ppc.mean(axis=0)\n",
    "lo, hi     = np.quantile(ppc, [0.025, 0.975], axis=0)  # 95% band"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49b22d5-8847-43be-9887-3a4c363a7a78",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Error metrics (raw points & summary)\n",
    "def rmse_matrix(S, O, mask=None):\n",
    "    S = np.asarray(S, float); O = np.asarray(O, float)\n",
    "    if mask is not None:\n",
    "        M = np.asarray(mask, bool)\n",
    "        diff2 = (S - O)**2\n",
    "        diff2 = diff2[M]\n",
    "        return np.sqrt(diff2.mean())\n",
    "    return np.sqrt(np.mean((S - O)**2))\n",
    "\n",
    "def mape_matrix(S, O, eps=1e-12, mask=None):\n",
    "    S = np.asarray(S, float); O = np.asarray(O, float)\n",
    "    denom = np.where(np.abs(O) < eps, np.nan, O)  # ignore near-zero obs\n",
    "    err = np.abs((S - O) / denom) * 100.0\n",
    "    if mask is not None:\n",
    "        err = err[np.asarray(mask, bool)]\n",
    "    return np.nanmean(err)\n",
    "\n",
    "def smape_matrix(S, O, eps=1e-12, mask=None):\n",
    "    S = np.asarray(S, float); O = np.asarray(O, float)\n",
    "    denom = np.maximum(eps, (np.abs(S) + np.abs(O)) / 2)\n",
    "    err = 100.0 * np.abs(S - O) / denom\n",
    "    if mask is not None:\n",
    "        err = err[np.asarray(mask, bool)]\n",
    "    return np.mean(err)\n",
    "\n",
    "def frobenius_norm(S, O):\n",
    "    # Equivalent to sqrt of sum of squares; if you want RMSE, divide by sqrt(S.size)\n",
    "    return np.linalg.norm(S - O, ord='fro')\n",
    "\n",
    "def rmse_per_time(S, O):\n",
    "    # returns a vector of RMSE(t) then a scalar average\n",
    "    diff = (S - O)**2\n",
    "    rmse_t = np.sqrt(diff.mean(axis=0))\n",
    "    return rmse_t, float(rmse_t.mean())\n",
    "\n",
    "def rmse_per_strain(S, O):\n",
    "    diff = (S - O)**2\n",
    "    rmse_s = np.sqrt(diff.mean(axis=1))\n",
    "    return rmse_s, float(rmse_s.mean())\n",
    "def rmsd(a, b):\n",
    "    a = np.asarray(a,float); b = np.asarray(b,float)\n",
    "    return np.sqrt(np.mean((a-b)**2))\n",
    "\n",
    "def mape(y_true, y_pred, eps=1e-12):\n",
    "    y_true = np.asarray(y_true,float)\n",
    "    y_pred = np.asarray(y_pred,float)\n",
    "    denom = np.where(np.abs(y_true) < eps, np.nan, y_true)\n",
    "    return np.nanmean(np.abs((y_pred - y_true)/denom)) * 100.0\n",
    "\n",
    "# pointwise fit metrics\n",
    "rmsd_map  = rmsd(y_obs_array_scaled, y_hat_map)\n",
    "mape_map  = mape(y_obs_array_scaled, y_hat_map)\n",
    "rmsd_mean = rmsd(y_obs_array_scaled, y_hat_mean)\n",
    "mape_mean = mape(y_obs_array_scaled, y_hat_mean)\n",
    "\n",
    "# summary-space metrics (optional)\n",
    "s_obs     = my_summary(y_obs_array_scaled)\n",
    "s_map     = my_summary(y_hat_map)\n",
    "s_mean    = my_summary(y_hat_mean)\n",
    "rmsd_sum_map  = rmsd(s_obs, s_map)\n",
    "rmsd_sum_mean = rmsd(s_obs, s_mean)\n",
    "\n",
    "print(f\"MAP fit:   RMSD={rmsd_map:.4g},  MAPE={mape_map:.3g}%  | summary RMSD={rmsd_sum_map:.4g}\")\n",
    "print(f\"Mean fit:  RMSD={rmsd_mean:.4g}, MAPE={mape_mean:.3g}% | summary RMSD={rmsd_sum_mean:.4g}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a301bc0-2a8a-43c8-a9e4-aa39b37cecee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_heatmap(M, fname=\"heatmap_v4.png\", vmax=None):\n",
    "    M = np.asarray(M, float)\n",
    "    plt.figure(figsize=(7.5,5))\n",
    "    im = plt.imshow(M, aspect='auto', origin='upper',\n",
    "                    interpolation='nearest', cmap='viridis',\n",
    "                    vmin=np.nanmin(M), vmax=vmax)\n",
    "    plt.colorbar(im, label=\"count\")\n",
    "    plt.xlabel(\"time\"); plt.ylabel(\"strain\")\n",
    "    plt.title(\"Strain × time heatmap\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(fname, dpi=160)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "plot_heatmap(y_hat_map, fname=\"experimental_data/SSPrev_heatmap_v4.png\")\n",
    "plot_heatmap(y_obs_array_scaled, fname=\"experimental_data/SSPrev_heatmap_obs_v4.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357f8512-d763-430c-ab9c-90dd6adec821",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ppc has shape (n_draws, n_strains, n_times)\n",
    "ppc_mean = ppc.mean(axis=0)\n",
    "ppc_lo, ppc_hi = np.quantile(ppc, [0.025, 0.975], axis=0)\n",
    "ppc_mean = y_hat_mean\n",
    "ppc_lo = lo\n",
    "ppc_hi = hi\n",
    "plot_heatmap(ppc_mean, fname=\"experimental_data/ppc_mean_heatmap_v4.png\")\n",
    "plot_heatmap(ppc_lo,   fname=\"experimental_data/ppc_lo_heatmap_v4.png\")\n",
    "plot_heatmap(ppc_hi,   fname=\"experimental_data/ppc_hi_heatmap_v4.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a6ba9a-5bc0-4866-9c7c-8a16b2fa1891",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8470a4-27a4-4c9d-9503-6f3e42a76fec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd919854-adf4-40c7-8b19-258ba1abfb07",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pylfire1_env]",
   "language": "python",
   "name": "conda-env-pylfire1_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
